# This is a CITATION.cff file adhering to the Citation File Format schema version 1.2.0
# For more information, see: https://citation-file-format.github.io/

cff-version: 1.2.0
message: "If you use this work, please cite it using the metadata below."

title: "Choose Your Model Size: Any Compression by a Single Gradient Descent"
authors:
  - given-names: "M."
    family-names: "Genzel"
  - given-names: "P."
    family-names: "Putzky"
  - given-names: "P."
    family-names: "Zhao"
  - given-names: "S."
    family-names: "Schulze"
  - given-names: "M."
    family-names: "Mollenhauer"
  - given-names: "R."
    family-names: "Seidel"
  - given-names: "S."
    family-names: "Dietzel"
  - given-names: "T."
    family-names: "Wollmann"

identifiers:
  - type: arxiv
    value: "2502.01717"
    description: "arXiv preprint identifier"

date-released: 2025-02
url: "https://arxiv.org/abs/2502.01717"
repository-code: "https://github.com/MerantixMomentum/acip"
license: "Apache-2.0"
keywords:
  - "model compression"
  - "network pruning"
  - "deep learning"
  - "foundation models"
abstract: "The adoption of Foundation Models in resource-constrained environments remains challenging due to their large size and inference costs. A promising way to overcome these limitations is post-training compression, which aims to balance reduced model size against performance degradation. This work presents Any Compression via Iterative Pruning (ACIP), a novel algorithmic approach to determine a compression-performance trade-off from a single stochastic gradient descent run. To ensure parameter efficiency, we use an SVD-reparametrization of linear layers and iteratively prune their singular values with a sparsity-inducing penalty. The resulting pruning order gives rise to a global parameter ranking that allows us to materialize models of any target size. Importantly, the compressed models exhibit strong predictive downstream performance without the need for costly fine-tuning. We evaluate ACIP on a large selection of open-weight LLMs and tasks, and demonstrate state-of-the-art results compared to existing factorisation-based compression methods. We also show that ACIP seamlessly complements common quantization-based compression techniques."
